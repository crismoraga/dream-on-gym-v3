# DREAM-ON-GYM V3 ‚Äî ULTRA (Optimized, Enterprise-ready)

 [![License](https://img.shields.io/badge/license-MIT-blue.svg)](licenses/LICENSE_GYM.md)  [![Python](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org)  [![Status](https://img.shields.io/badge/status-experimental-orange.svg)](https://github.com/crismoraga/dream-on-gym-v3)

 [![Docs](https://img.shields.io/badge/Docs-View-blue)](docs/docs/index.md)  [![Examples](https://img.shields.io/badge/Examples-Run-green)](examples/gym/example1.py)  [![Run Quick Eval](https://img.shields.io/badge/QuickEval-run-orange)](dreamongymv2/reward_functions/quick_evaluation.py)  [![Status](https://img.shields.io/badge/status-GitHub-blue)](https://github.com/crismoraga/dream-on-gym-v3)

<!-- Banner -->
![DREAM-ON-GYM-V3 Banner](assets/banner.svg)

*Nota: Si tu cliente no renderiza SVG, existe una versi√≥n PNG en `assets/banner.png`.*

<!-- Icon: use Markdown image - PNG fallback exists in assets/icon.png -->
![DREAM-ON-GYM V3 icon](assets/icon.svg)

---

Un framework avanzado de investigaci√≥n y desarrollo para entrenamiento por Deep Reinforcement Learning (DRL) en Elastic Optical Networks (EON). DREAM-ON-GYM-V3 es la evoluci√≥n y reingenier√≠a de **DREAM-ON-GYM-V2**, aportando mayor estabilidad, reproducibilidad, y un conjunto completo de utilidades para evaluaci√≥n y producci√≥n: reward engineering avanzado, pipelines de experimento y dashboards listos para usar.

> ¬øPor qu√© V3? V3 est√° dise√±ado para investigadores y equipos que necesitan reproducibilidad, comparativa experimental y producci√≥n: mejores m√©tricas, integraci√≥n con SB3, evaluaciones automatizadas y visualizaci√≥n profesional.

---

## üìö √çndice

- Resumen y Valor
- Que hay de nuevo en V3
- Caracteristicas principales
- Instalacion rapida
- Quickstart / Demo
- Ejemplos y Entrenamiento (PPO)
- Arquitectura & Diagrama Mermaid
- Metricas & Reward Engineering
- Evaluacion y Pipelines reproducibles
- Migracion V2 ‚Üí V3 (guia rapida)
- Contribuir y contactos
- Changelog & Licencia

---

## üìå Resumen & Valor

DREAM-ON-GYM-V3 es una plataforma de investigaci√≥n enfocada en:

- Entrenamiento de agentes DRL sobre una simulaci√≥n realista de redes √≥pticas el√°sticas.
- Evaluaci√≥n reproducible de reward functions (incluye una `SpectralEntropyAdaptiveReward` novedosa basada en entrop√≠a espectral de la red).
- F√°cil integraci√≥n con Stable-Baselines3 y Gymnasium para experimentos cient√≠ficos y pruebas de producci√≥n.

Valor diferencial:

- Reproducibilidad: `run_experiments.py` exporta JSON y PNG; seeds configurables.
- Reusable experiments: scripts parametrizados para topolog√≠as NSFNet, GermanNet, ItalianNet, y m√°s.
- Integraci√≥n lista para pipelines empresariales: CI/CD, benchmarks y visualizaci√≥n interactiva.

---

## üîÑ Que hay de nuevo en V3

| √Årea | DREAM-ON-GYM-V2 | DREAM-ON-GYM-V3 (ULTRA) |
|------|------------------|-------------------------|
| Core | Implementaciones iniciales y ejemplos | Reingenier√≠a: modularidad, estabilidad y testeo reproducible |
| Reward functions | B√°sicos (+1/-1) | 5 rewards: Baseline, QoT-Aware, Multi-Objective, Fragmentation-Aware, SpectralEntropyAdaptiveReward (NOVEL) |
| Evaluaci√≥n | Scripts seccionados | `quick_evaluation`, `full_evaluation`, `run_experiments` (pipelines reproducibles) |
| Visualizaci√≥n | B√°sica | Radar, Heatmaps, Boxplots, distribution plots + dashboard Plotly |
| Integraci√≥n | Stable-Baselines (limitado) | Gymnasium + Stable-Baselines3 + sb3-contrib, compatible con PyTorch/TF backends |
| Reproducibilidad | Parcial | Resultados exportables (JSON/PNG) y configuraci√≥n determinista (seeds) |

---

## ‚öôÔ∏è Caracter√≠sticas principales

- Soporte full EON con multi-bandas (L,C,S,E,O) y slots por enlace.
- Reward engineering avanzado: QoT-aware, fragmentation-sensitive y adaptativo por entrop√≠a.
- Funciones de m√©tricas integradas en `metrics.py` (fragmentation ratio, entropy, utilization, QoT estimators).
- Scripts de evaluaci√≥n (r√°pidos y completos) y dashboards con visualizaciones autom√°ticas.

---

## üíæ Instalaci√≥n r√°pida

Recomendado: Python 3.10, entornos virtuales. Ejemplo PowerShell (Windows):

```powershell
python -m venv .venv310
. .\.venv310\Scripts\Activate.ps1
pip install -U pip setuptools wheel
pip install -e .
```

Dependencias principales: numpy, gymnasium, stable-baselines3, sb3-contrib, tensorflow (opcional), matplotlib, pandas, mpi4py.

Para GPU: instale la versi√≥n de `tensorflow` y `torch` con soporte CUDA acorde a su entorno.

---

## ‚ú® Quickstart ‚Äî Demo y evaluaci√≥n r√°pida

1. Demo interactiva (evaluaci√≥n y ejemplos):

```powershell
python -m dreamongymv2.reward_functions.demo
```

1. Quick evaluation con generaci√≥n de gr√°ficos:

```powershell
python -m dreamongymv2.reward_functions.quick_evaluation
```

1. Evaluaci√≥n completa con simulaciones y reporte:

```powershell
python -m dreamongymv2.reward_functions.full_evaluation
```

1. Ejecutar ejemplo de entrenamiento (PPO) ‚Äî ver `examples/gym`:

```powershell
python -m dreamongymv2.reward_functions.examples
```

---

## üß™ Ejemplo de entrenamiento (PPO)

Snippet m√≠nimo con Stable-Baselines3:

```python
from stable_baselines3 import PPO
from dreamongymv2.gym_basic.envs.rl_on_env import RlOnEnv
from dreamongymv2.reward_functions import MultiObjectiveReward

env = RlOnEnv(reward_fn=MultiObjectiveReward())
model = PPO('MlpPolicy', env, verbose=1)
model.learn(total_timesteps=10000)
model.save('ppo_multiobj_v3')
```

> Nota: Revisa `examples.py` para ejecuciones reproducibles con seeds y checkpoints.

---

## üèó Arquitectura (visi√≥n general)

```mermaid
flowchart LR
    subgraph SIMULATION
        Sim["Simulator (simNetPy)"] -->|Events and State| Network["Network (links, slots)"]
        Network -->|Spectrum State| Metrics["metrics.py"]
    end
    subgraph RL
        Env["RlOnEnv"] -->|Observations| Agent["RL Agent"]
        Agent -->|Actions| Env
        Env -->|Invokes| RewardFns["Reward Functions Module"]
    end
    Metrics -->|features / entropy| RewardFns
    RewardFns -->|Reward| Env
    Agent -->|Training Logs| Logger["Trainer & Logger"]
    Logger -->|Plots / Stats| Dashboard["Plotly Dashboard"]
```

---

## üßÆ Metricas y Reward Engineering

- El m√≥dulo `metrics.py` provee:

- Fragmentation metrics: external, internal (Shannon entropy) y average-block size
- Network utilization: por enlace y promedio de red
- QoT estimators: OSNR heuristics para estimar QoT

Reward functions implementadas:

1. `BaselineReward`: +1 asignado / -1 bloqueado
2. `QoTAwareReward`: integra OSNR y distancia (penaliza enlaces largos)
3. `MultiObjectiveReward`: combinaci√≥n ponderada (blocking, fragmentation, throughput)
4. `FragmentationAwareReward`: penaliza fragmentaci√≥n externa e interna
5. `SpectralEntropyAdaptiveReward` (NOVEL): usa entrop√≠a de Shannon para zonificar la red y ajustar bonificaciones/penalizaciones

---

## üìä Evaluacion y Pipelines reproducibles

- `quick_evaluation.py`: run rapide, guarda PNG en `reward_functions/plots`
- `full_evaluation.py`: corre el simulador para m√∫ltiples cargas y topolog√≠as, guarda JSON y plots
- `run_experiments.py`: pipeline completo para replicar experimentos (multi-topolog√≠as, œÅs, repeticiones)

**Reproducibilidad:** Configure seeds antes de `sim.init()` para garantizar resultados determin√≠sticos (ej. `sim.setSeedArrive(42)` etc.).

---

## üî¨ Migracion V2 ‚Üí V3 ‚Äî Guia r√°pida

Si vienes desde V2, los puntos clave:

- API de reward: ahora el `calculate()` acepta `allocated` y `network` (m√°s rico en contexto).
- Los scripts de evaluaci√≥n (`quick_evaluation`, `full_evaluation`) reemplazan pruebas ad-hoc y facilitan benchmarking.
- `SpectralEntropyAdaptiveReward` es una funci√≥n nueva que exige `metrics.get_network_spectrum_state(network)` o equivalente.
- Para migrar wrappers: aseg√∫rate de inyectar la instancia `reward_fn` al crear el env: `RlOnEnv(reward_fn=SpectralEntropyAdaptiveReward())`.

---

## üì∑ Visualizaciones y c√≥mo crear GIFs

- Los plots generados se guardan en `dreamongymv2/reward_functions/plots`. Ejemplos:

- `blocking_probability.png`
- `rewards.png`
- `radar.png` (comparativa multidimensional)

Para crear GIFs a partir de PNGs (ImageMagick / ffmpeg):

```bash
magick convert -delay 20 -loop 0 plots/radar_*.png plots/radar.gif
# or
ffmpeg -framerate 10 -pattern_type glob -i 'plots/radar_*.png' -vf "scale=800:-1" plots/radar.gif
```

---

## üß∞ Para integraciones enterprise

- Use `run_experiments.py` y exporte JSON + PNG como artefactos para su pipeline CI/CD.
- Para entrenamiento intensivo, configure GPU y entorno reproducible.
- Para despliegues research ‚Üí production: validar reproducibilidad, tests y m√©tricas en cada release.

---

## üë©‚Äçüíª Contribuir

1. Fork & branch: `git checkout -b feature/mi-feature`
2. Agrega tests reproducibles y un ejemplo m√≠nimo
3. Abre PR con descripci√≥n, m√©tricas y artefactos (plots, CSV, JSON)
4. Ejecuta `pytest` y valida ejemplos de `examples/gym`

---

## üìú Changelog & Licencia

- **v3.0.0** ‚Äî Reingenier√≠a, 5 reward functions, pipelines y dashboards
- **v2.0.0** ‚Äî Implementaci√≥n inicial: ejemplos y la conexi√≥n inicial con Flex-Net-Sim

Licencias: revisa `licenses/` para los detalles de licencias incluidas (Flex-Net-Sim y otros componentes)

---
