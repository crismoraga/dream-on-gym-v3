# DREAM-ON-GYM-V2: MÃ³dulo de Funciones de Recompensa

## ğŸ“‹ Resumen del Proyecto

Este mÃ³dulo implementa **5 funciones de recompensa** para entrenamiento de agentes de Deep Reinforcement Learning (DRL) en redes Ã³pticas elÃ¡sticas (EON), incluyendo una **propuesta novedosa** basada en entropÃ­a espectral.

## ğŸ¯ Funciones Implementadas

| # | FunciÃ³n | Tipo | DescripciÃ³n |
|---|---------|------|-------------|
| 1 | `BaselineReward` | Binaria | +1 Ã©xito, -1 bloqueo (referencia) |
| 2 | `QoTAwareReward` | QoT | Considera OSNR y distancia de transmisiÃ³n |
| 3 | `MultiObjectiveReward` | Multi-objetivo | CombinaciÃ³n ponderada de mÃ©tricas |
| 4 | `FragmentationAwareReward` | FragmentaciÃ³n | Penaliza fragmentaciÃ³n espectral |
| 5 | `SpectralEntropyAdaptiveReward` | **NOVEL** | Basada en entropÃ­a de Shannon |

## ğŸ“ Estructura del MÃ³dulo

```
dreamongymv2/reward_functions/
â”œâ”€â”€ __init__.py              # Exports del mÃ³dulo
â”œâ”€â”€ reward_functions.py      # Clases de recompensa (5 implementaciones)
â”œâ”€â”€ metrics.py               # CÃ¡lculos de fragmentaciÃ³n, utilizaciÃ³n, QoT
â”œâ”€â”€ examples.py              # Ejemplos de uso (4 ejemplos)
â”œâ”€â”€ demo.py                  # Demo interactiva
â”œâ”€â”€ quick_evaluation.py      # EvaluaciÃ³n rÃ¡pida
â”œâ”€â”€ full_evaluation.py       # EvaluaciÃ³n con simulador completo
â”œâ”€â”€ run_experiments.py       # Script de experimentos
â”œâ”€â”€ evaluate_rewards.py      # EvaluaciÃ³n con estadÃ­sticas
â”œâ”€â”€ dashboard.py             # Dashboard interactivo (Plotly)
â”œâ”€â”€ DOCUMENTATION.md         # DocumentaciÃ³n matemÃ¡tica completa
â””â”€â”€ plots/                   # Visualizaciones generadas
    â”œâ”€â”€ blocking_probability.png
    â”œâ”€â”€ rewards.png
    â”œâ”€â”€ fragmentation.png
    â”œâ”€â”€ evolution.png
    â”œâ”€â”€ radar.png
    â”œâ”€â”€ heatmap.png
    â”œâ”€â”€ GermanNet_*.png
    â””â”€â”€ ItalianNet_*.png
```

## ğŸš€ Uso RÃ¡pido

```python
from dreamongymv2.reward_functions import (
    BaselineReward,
    SpectralEntropyAdaptiveReward,
    RewardFactory
)

# Crear funciÃ³n de recompensa
reward_fn = SpectralEntropyAdaptiveReward()

# Calcular recompensa
reward = reward_fn.calculate(
    allocated=True,
    network=network,  # Objeto Network del simulador
)

# O usar factory
reward_fn = RewardFactory.create('spectral_entropy')
```

## ğŸ“ Formulaciones MatemÃ¡ticas

### 1. Baseline Reward
```
R = +1  si conexiÃ³n asignada
R = -1  si conexiÃ³n bloqueada
```

### 2. QoT-Aware Reward
```
R = w_base Ã— R_base + w_qot Ã— (OSNR_est / OSNR_thresh) + w_dist Ã— (1 - d/d_max)
```

### 3. Multi-Objective Reward
```
R = Î£(w_i Ã— R_i)  donde i âˆˆ {blocking, fragmentation, throughput}
Pesos default: w_block=0.5, w_frag=0.2, w_tput=0.3
```

### 4. Fragmentation-Aware Reward
```
R = R_base - Î± Ã— F_external - Î² Ã— F_internal + Î³ Ã— (1 - F_total)
Donde F_external = bloques_libres/total_slots
```

### 5. Spectral-Entropy Adaptive Reward (NOVEL)
```
H(S) = -Î£ p_i Ã— logâ‚‚(p_i)  (EntropÃ­a de Shannon)

Zonas adaptativas:
- Baja (H < 0.3):    R = R_base + 0.15  (red casi vacÃ­a)
- Media (0.3-0.6):   R = R_base + 0.05  (operaciÃ³n normal)
- Alta (0.6-0.8):    R = R_base - 0.10  (precauciÃ³n)
- CrÃ­tica (H > 0.8): R = R_base - 0.25  (saturaciÃ³n)
```

## ğŸ“Š EjecuciÃ³n de Evaluaciones

```bash
# Demo interactiva
python -m dreamongymv2.reward_functions.demo

# EvaluaciÃ³n rÃ¡pida con grÃ¡ficos
python -m dreamongymv2.reward_functions.quick_evaluation

# EvaluaciÃ³n completa con simulador
python -m dreamongymv2.reward_functions.full_evaluation

# Ejemplos de uso
python -m dreamongymv2.reward_functions.examples
```

## ğŸ“ˆ Visualizaciones Generadas

El mÃ³dulo genera automÃ¡ticamente:
- GrÃ¡ficos de Blocking Probability vs Carga
- Comparativas de recompensa promedio
- DistribuciÃ³n de recompensas (boxplots)
- Heatmaps de rendimiento
- Radar charts multidimensionales
- Curvas de evoluciÃ³n temporal

## ğŸ”¬ Referencias BibliogrÃ¡ficas

1. **DeepRMSA**: Chen et al., "DeepRMSA: A Deep RL Framework for EON", JLT 2019
2. **QoT-Aware DRL**: Salami et al., "QoT-Aware Resource Allocation", JOCN 2020
3. **Multi-Band DRL**: Etezadi et al., "Multi-Band EON with DRL", ECOC 2021
4. **FragmentaciÃ³n**: Wright et al., "Fragmentation-Aware RMSA", OFC 2015

## âœ… Estado del Proyecto

- [x] ImplementaciÃ³n de 5 funciones de recompensa
- [x] MÃ³dulo de mÃ©tricas (fragmentaciÃ³n, utilizaciÃ³n, QoT)
- [x] Ejemplos de uso documentados
- [x] IntegraciÃ³n con simulador Flex-Net-Sim
- [x] GeneraciÃ³n de visualizaciones
- [x] DocumentaciÃ³n matemÃ¡tica completa
- [x] Demo interactiva
- [ ] Entrenamiento comparativo con PPO/DQN
- [ ] Dashboard web interactivo

## ğŸ‘¨â€ğŸ’» Autor

DREAM-ON-GYM-V2 Team - 2024
