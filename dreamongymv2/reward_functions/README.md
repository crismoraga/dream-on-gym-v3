# DREAM-ON-GYM-V3 ULTRA: MÃ³dulo de Funciones de Recompensa

[![Docs](https://img.shields.io/badge/Docs-View-blue)](DOCUMENTATION.md)
[![Examples](https://img.shields.io/badge/Examples-Run-green)](examples.py)
[![Try Quick Eval](https://img.shields.io/badge/Try%20Quick%20Eval-run-orange)](quick_evaluation.py)

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](../../licenses/)
[![Python](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org)
[![Status](https://img.shields.io/badge/status-experimental-orange.svg)](https://github.com/crismoraga/dream-on-gym-v3)

## ğŸ“‹ Resumen del Proyecto

Este mÃ³dulo es la evoluciÃ³n de DREAM-ON-GYM-V2 y ahora forma parte de la **versiÃ³n DREAM-ON-GYM-V3 (Ultra, optimizada y mejorada)**. EstÃ¡ diseÃ±ado para facilitar experimentos reproducibles y comparaciones rigurosas de estrategias de reward engineering en redes Ã³pticas elÃ¡sticas (EON).

En v3 nos centramos en estabilidad, reusabilidad y en un conjunto de utilidades para benchmarking de RL en EON: implementamos 5 reward functions, mÃ©tricas avanzadas, scripts de evaluaciÃ³n y dashboards interactivos para comparar enfoques.

> Nota: Este README describe la versiÃ³n del paquete de `reward_functions` dentro del release DREAM-ON-GYM-V3. Si necesitas la integraciÃ³n completa con el paquete raÃ­z, revisa la documentaciÃ³n en `docs/`.

---

## ğŸ“š Ãndice (Table of Contents)

- Resumen del Proyecto
- QuÃ© hay de nuevo (v3)
- Funciones Implementadas
- Estructura del MÃ³dulo
- InstalaciÃ³n & Quickstart
- Ejemplos & Demostraciones
- Evaluaciones y Scripts
- Visualizaciones y Reportes
- Arquitectura (Mermaid)
- MÃ©tricas disponibles
- MigraciÃ³n V2 â†’ V3
- Contribuir
- Licencia & Contacto

---

## ğŸ¯ Funciones Implementadas

| # | FunciÃ³n | Tipo | DescripciÃ³n |
|---|---------|------|-------------|
| 1 | `BaselineReward` | Binaria | +1 Ã©xito, -1 bloqueo (referencia) |
| 2 | `QoTAwareReward` | QoT | Considera OSNR y distancia de transmisiÃ³n |
| 3 | `MultiObjectiveReward` | Multi-objetivo | CombinaciÃ³n ponderada de mÃ©tricas |
| 4 | `FragmentationAwareReward` | FragmentaciÃ³n | Penaliza fragmentaciÃ³n espectral |
| 5 | `SpectralEntropyAdaptiveReward` | **NOVEL** | Basada en entropÃ­a de Shannon; adaptive reward que ajusta bonificaciones y penalizaciones segÃºn la entropÃ­a espectral de la red |

---


ğŸ‰ Pro-tip: Si vienes de v2, en la secciÃ³n "QuÃ© hay de nuevo (v3)" mÃ¡s abajo verÃ¡s las ventajas principales y la guÃ­a de migraciÃ³n.

---

<a name="que-hay-de-nuevo-v3"></a>
## ğŸ”„ QuÃ© hay de nuevo (v3)

DREAM-ON-GYM-V3 es una versiÃ³n de reingenierÃ­a: no es solo mÃ¡s funciones, es una **evoluciÃ³n de arquitectura** con soporte mejorado para benchmarking y reproducibilidad.

| CategorÃ­a | v2 | v3 (ULTRA) |
|-----------|----|------------|
| Reward Engineering | 1-2 reward functions | 5 funciones (QoT, Fragmentation, Multi-objective, SpectralEntropy NOVEL, Baseline) |
| Evaluaciones | manual / ejemplos | `quick_evaluation`, `full_evaluation`, `run_experiments` pipelines |
| Visualizaciones | estÃ¡ticas | Radar, Heatmaps, Boxplots, Plots automÃ¡ticos y dashboard interactivo |
| Integraciones | Gym/basic | Gymnasium, Stable-Baselines3 + sb3-contrib compatibles |
| Reproductibilidad | limitada | JSON export, reproducible reports, deterministic sim settings |

### MigraciÃ³n rÃ¡pida: puntos clave

- Cambios de API: `calculate()` ahora acepta `allocated` y `network`. Revisa cÃ³mo tu wrapper inyecta el `reward_fn` al crear el env.
- Revisa `DOCUMENTATION.md` para los parÃ¡metros y la configuraciÃ³n por defecto de cada reward.

---

<a name="arquitectura-mermaid"></a>
## ğŸ—ï¸ Arquitectura (VisiÃ³n general)

```mermaid
flowchart LR
        subgraph Simulation
            Sim[Simulator (simNetPy)] -->|Events & State| Network[Network (links, slots)]
            Network -->|spectrum state| Metrics[metrics.py]
        end
        subgraph RL
            Env[Gym Env (RlOnEnv)] -->|obs| Agent[RL Agent]
            Agent -->|actions| Env
            Env -->|invokes| RewardFns[Reward Functions]
        end
        Metrics -->|features/entropy| RewardFns
        RewardFns -->|reward| Env
        Agent -->|training| Logger[Training & Logger]
        Logger -->|plots/stats| Dashboard[Plotly Dashboard]
```


<a name="estructura-del-modulo"></a>
## ğŸ“ Estructura del MÃ³dulo

```text
dreamongymv2/reward_functions/         # Paquete principal con lÃ³gica de reward
â”œâ”€â”€ __init__.py              # Exports del mÃ³dulo
â”œâ”€â”€ reward_functions.py      # Clases de recompensa (5 implementaciones)
â”œâ”€â”€ metrics.py               # CÃ¡lculos de fragmentaciÃ³n, utilizaciÃ³n, QoT
â”œâ”€â”€ examples.py              # Ejemplos de uso (4 ejemplos)
â”œâ”€â”€ demo.py                  # Demo interactiva
â”œâ”€â”€ quick_evaluation.py      # EvaluaciÃ³n rÃ¡pida
â”œâ”€â”€ full_evaluation.py       # EvaluaciÃ³n con simulador completo
â”œâ”€â”€ run_experiments.py       # Script de experimentos
â”œâ”€â”€ evaluate_rewards.py      # EvaluaciÃ³n con estadÃ­sticas
â”œâ”€â”€ dashboard.py             # Dashboard interactivo (Plotly)
â”œâ”€â”€ DOCUMENTATION.md         # DocumentaciÃ³n matemÃ¡tica completa
â””â”€â”€ plots/                   # Visualizaciones generadas
    â”œâ”€â”€ blocking_probability.png
    â”œâ”€â”€ rewards.png
    â”œâ”€â”€ fragmentation.png
    â”œâ”€â”€ evolution.png
    â”œâ”€â”€ radar.png
    â”œâ”€â”€ heatmap.png
    â”œâ”€â”€ GermanNet_*.png
    â””â”€â”€ ItalianNet_*.png
```

<a name="uso-rapido"></a>
## ğŸš€ Uso RÃ¡pido

```python
from dreamongymv2.reward_functions import (
    BaselineReward,
    SpectralEntropyAdaptiveReward,
    RewardFactory
)

# Crear funciÃ³n de recompensa
reward_fn = SpectralEntropyAdaptiveReward()

# Calcular recompensa
reward = reward_fn.calculate(
    allocated=True,
    network=network,  # Objeto Network del simulador
)

# O usar factory
reward_fn = RewardFactory.create('spectral_entropy')
```

## ğŸ“ Formulaciones MatemÃ¡ticas

### 1. Baseline Reward

```text
R = +1  si conexiÃ³n asignada
R = -1  si conexiÃ³n bloqueada
```

### 2. QoT-Aware Reward

```text
R = w_base Ã— R_base + w_qot Ã— (OSNR_est / OSNR_thresh) + w_dist Ã— (1 - d/d_max)
```

### 3. Multi-Objective Reward

```text
R = Î£(w_i Ã— R_i)  donde i âˆˆ {blocking, fragmentation, throughput}
Pesos default: w_block=0.5, w_frag=0.2, w_tput=0.3
```

### 4. Fragmentation-Aware Reward

```text
R = R_base - Î± Ã— F_external - Î² Ã— F_internal + Î³ Ã— (1 - F_total)
Donde F_external = bloques_libres/total_slots
```

### 5. Spectral-Entropy Adaptive Reward (NOVEL)

```text
H(S) = -Î£ p_i Ã— logâ‚‚(p_i)  (EntropÃ­a de Shannon)

Zonas adaptativas:
- Baja (H < 0.3):    R = R_base + 0.15  (red casi vacÃ­a)
- Media (0.3-0.6):   R = R_base + 0.05  (operaciÃ³n normal)
- Alta (0.6-0.8):    R = R_base - 0.10  (precauciÃ³n)
- CrÃ­tica (H > 0.8): R = R_base - 0.25  (saturaciÃ³n)
```

<a name="ejecucion-de-evaluaciones"></a>
## ğŸ“Š EjecuciÃ³n de Evaluaciones

```bash
# Demo interactiva
python -m dreamongymv2.reward_functions.demo

# EvaluaciÃ³n rÃ¡pida con grÃ¡ficos
python -m dreamongymv2.reward_functions.quick_evaluation

# EvaluaciÃ³n completa con simulador
python -m dreamongymv2.reward_functions.full_evaluation

# Ejemplos de uso
python -m dreamongymv2.reward_functions.examples
```

<a name="visualizaciones-y-reportes"></a>
## ğŸ“ˆ Visualizaciones Generadas

El mÃ³dulo genera automÃ¡ticamente:

- GrÃ¡ficos de Blocking Probability vs Carga
- Comparativas de recompensa promedio
- DistribuciÃ³n de recompensas (boxplots)
- Heatmaps de rendimiento
- Radar charts multidimensionales
- Curvas de evoluciÃ³n temporal

### ğŸ¨ GalerÃ­a (previews)

| Blocking Probability | Rewards | Radar |
|----------------------|---------|-------|
| ![BP](/dreamongymv2/reward_functions/plots/blocking_probability.png) | ![Rewards](/dreamongymv2/reward_functions/plots/rewards.png) | ![Radar](/dreamongymv2/reward_functions/plots/radar.png) |

---

<a name="reproducibilidad"></a>
## ğŸ”’ Reproducibilidad y Seeds

Para ejecutar experimentos reproducibles, define seeds en el simulador **antes** de llamar `init()`:

```python
from dreamongymv2.simNetPy.simulator_finite import Simulator

sim = Simulator(network_file, routes_file, "")
sim.setSeedArrive(42)
sim.setSeedDeparture(43)
sim.setSeedSrc(44)
sim.setSeedDst(45)
sim.init()
```

Adicionalmente, fija la semilla para Python y NumPy para reproducibilidad de experimentos RL:

```python
import random, numpy as np
random.seed(42)
np.random.seed(42)
```

---

<a name="metricas-disponibles"></a>
## ğŸ§® MÃ©tricas disponibles

El mÃ³dulo `metrics.py` ofrece las principales mÃ©tricas ya implementadas, entre otras:

- `calculate_fragmentation_ratio(link_slots, method='external|internal|average')`
- `get_network_spectrum_state(network)` â†’ retorna dict con `avg_fragmentation`, `avg_utilization`, `entropy`, etc.
- QoT estimators (OSNR-based)

Usa estas funciones para instrumentar recompensas personalizadas y dashboards.

---

<a name="optimizaciÃ³n-produccion"></a>
## âš¡ OptimizaciÃ³n y producciÃ³n (enterprise)

- Usa `run_experiments.py` con `--parallel` (si lo habilitas) para ejecutar mÃºltiples configuraciones en paralelo.
- Configura entornos con GPU para entrenamiento (PyTorch/TF) si usas enfoques basados en NN intensivos.
- Para pipelines de CI/CD: aÃ±ade tests con `pytest` y crea artefactos (JSON + PNG) por cada release.

### ğŸï¸ Crear GIFs/Animaciones a partir de grÃ¡ficos

Si deseas convertir una secuencia de PNG en un GIF animado (para presentaciones o dashboard), usa ImageMagick o `convert`:

```bash
# Ejemplo con ImageMagick (Windows o macOS con brew)
magick convert -delay 20 -loop 0 plots/radar_*.png plots/radar.gif
```

O usa `ffmpeg`:

```bash
ffmpeg -framerate 10 -pattern_type glob -i 'plots/radar_*.png' -vf "scale=800:-1" plots/radar.gif
```


## ğŸ”¬ Referencias BibliogrÃ¡ficas

1. **DeepRMSA**: Chen et al., "DeepRMSA: A Deep RL Framework for EON", JLT 2019
2. **QoT-Aware DRL**: Salami et al., "QoT-Aware Resource Allocation", JOCN 2020
3. **Multi-Band DRL**: Etezadi et al., "Multi-Band EON with DRL", ECOC 2021
4. **FragmentaciÃ³n**: Wright et al., "Fragmentation-Aware RMSA", OFC 2015

## âœ… Estado del Proyecto

- [x] ImplementaciÃ³n de 5 funciones de recompensa
- [x] MÃ³dulo de mÃ©tricas (fragmentaciÃ³n, utilizaciÃ³n, QoT)
- [x] Ejemplos de uso documentados
- [x] IntegraciÃ³n con simulador Flex-Net-Sim
- [x] GeneraciÃ³n de visualizaciones
- [x] DocumentaciÃ³n matemÃ¡tica completa
- [x] Demo interactiva
- [ ] Entrenamiento comparativo con PPO/DQN
- [ ] Dashboard web interactivo

## ğŸ‘¨â€ğŸ’» Autor

DREAM-ON-GYM-V2 Team - 2024

---

## ğŸ¤ CÃ³mo contribuir

Nos encantan las contribuciones. Para colaborar con DREAM-ON-GYM-V3:

1. Fork del repositorio
2. Crear una rama con tu feature: `git checkout -b feature/my-feature`
3. AÃ±adir pruebas o un ejemplo reproducible
4. Abrir PR con descripciÃ³n, benchmarks y grÃ¡ficos (si aplican)

Si trabajas en algoritmos RL, por favor incluye seed/fixed-config para reproducibilidad.

---

## ğŸ“œ Licencia & Contacto

El proyecto contiene mÃºltiples licencias, revisa `licenses/`. Las implementaciones nuevas estÃ¡n bajo MIT salvo que se indique lo contrario.

Si necesitas soporte o quieres colaborar en integraciones enterprise, abre un `issue` o contacta al equipo en `support@dreamongym.org`.

---

## ğŸ“¦ Changelog (Resumen rÃ¡pido)

- **v3.0.0** â€” (Hoy): ReorganizaciÃ³n del paquete, nuevo conjunto de 5 reward functions (incl. SpectralEntropyAdaptiveReward), pipelines reproducibles, dashboards y documentaciÃ³n extendida.
- **v2.0.0** â€” ImplementaciÃ³n original: ejemplos y la conexiÃ³n inicial con Flex-Net-Sim.

---

Gracias por usar DREAM-ON-GYM-V3 â€” si encuentras un bug o limitaciÃ³n, cuenta con nosotros para resolverlo.
